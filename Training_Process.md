# Abstract

We propose multiple stages converting Transformer based models to RWKV based models.

- Stage 1
  - Align RWKV's TimeMixer with Transformer's Self Attention. In this stage, only the TimeMixer will be updated. In this stage, ctx_length is set to 512.

- Stage 2
  - Align Hybrid model's logits with Transformer's logits. In this stage, the whole model will be updated. You can use larger model with the same vocab_size to do the distillation. In this stage, ctx_length is set to 512.

- Stage 3
  - Using longer ctx_length to do the distillation. In this stage, ctx_length is set to from 1024 to 8192 depending on the resoures you have.

- Stage 4
  - Using DPO to force the model to answer better.

# Steps

## Prepare the data
Stage 1,2,3 share the same format. For training purpose, the data should be processed to datasets with the following format:

```python
{
  "input_ids": input_ids,
  "labels": labels,
}
```
attention_mask will be generated in the training process automatically according the tokenizer you use.

The script to generate datasets from jsonl is 1_create_dataset.sh. This script supports both conversational and raw text data in jsonl format. You may need to modify the script to fit your data format.

## Configuration
There is a YAML file to control the training process. The example yaml configuration looks like:

```yaml
RWKV:
  layers: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63]
  grad_cp: 1

Llama:
  model_id: /home/yueyulin/models/Qwen2.5-7B-Instruct/
kl_weight: 1
ce_weight: 0
is_rwkv_att_only: True
teach_mode:
  is_client: False
  is_hidden_align: False
is_all_labels_kl: True
```
The RWKV.layers indicates which layers will be replaced by RWKV TimeMixer.
g
Currently is_rwkv_att_only and is_all_labels_kl must be set to True. We will NOT support other modes in the future.
is_rwkv_att_only means only the TimeMixer will be replaced in the DecoderLayer.
is_all_labels_kl means all the labels will be used to calculate the KL loss.

## Stage 1
You can use the 2_train_kl.sh to train the model in stage 1.

The example command is:
```bash
sh 2_train_kl.sh \
    -c configs/qwen_0.5b.yaml \
    -l 0.001 \
    -f 0.0001 \
    -b 8 \
    -p /home/yueyulin/data/all_train_ds_v1+magpie/ \
    -o /home/yueyulin/model/qwen_0.5b_full_layers \
    -g 1  \
    -s 1 \
    -R v6 \
    -S 2 \
    -t 400_000_000
```

The following table describes the parameters used in the training script:

| Parameter             | Description                                      | Default Value                        |
|-----------------------|--------------------------------------------------|--------------------------------------|
| `-c` CONFIG_FILE      | Path to the configuration file                   | `toys_playground/configs/qwen7B_KL_Local.yaml` |
| `-o` OUTPUT_DIR       | Directory to save the output                     | `toys_playground/output`             |
| `-p` PREPROCESSED_DATA| the dataset path generated by the script 1_create_dataset.sh                  | `toys_playground/dataset`            |
| `-n` NNODES           | Number of nodes                                  | `1`                                  |
| `-m` MAX_LENGTH       | Maximum sequence length                          | `512`                                |
| `-b` MICRO_BSZ        | Micro batch size                                 | `1`                                  |
| `-a` ACCUMULATE_GRAD_BATCHES | Number of gradient accumulation steps    | `4`                                  |
| `-l` LR_INIT          | Initial learning rate                            | `6e-4`                               |
| `-f` LR_FINAL         | Final learning rate                              | `1e-5`                               |
| `-w` WARMUP_STEPS     | Number of warmup steps                           | `50`                                 |
| `-k` CKPT_FILE        | Path to the checkpoint file                      | `""`                                 |
| `-g` GRAD_CP          | Gradient checkpointing                           | `1`                                  |
| `-d` DEEPSPEED_OFFLOAD| Enable DeepSpeed offloading                      | `""`                                 |
| `-F` FULL_PARAMS      | Use full parameters                              | `""`                                 |
| `-s` STAGE            | Training stage, you may pass 1,2,3                                   | `1`                                  |
| `-R` RWKV_VERSION     | RWKV version, you may pass v6 or v7                                     | `v6`                                 |
| `-W` WKV              | WKV environment variable, you may pass fla                         | `""`                                 |
| `-S` DEEPSPEED_STAGE  | Deepspeed stage, you may pass 2 or 3                                  | `3`                                  |
| `-t` MAX_TRAINED_TOKENS | Maximum number of trained tokens               | `100_000_000`                        |
| `-T` TERMINATE_LOSS   | Termination loss threshold                       | `0.01`                               |
| `-P` WANDB_PROJECT    | Weights & Biases project name                    | `hybrid_trainer_toys`                |
| `-W` WANDB            | Weights & Biases run name                        | `hybrid_trainer_toys`                |

## Stage 2

Stage 2 utilizes the model output checkpoint from stage 1 to train the model. 2_train_kl.sh can be used to train the model in stage 2 as well.

The example command is:
```bash
sh 2_train_kl.sh \
-c configs/qwen_0.5b.yaml \
-l 0.0001 \
-f 0.00001 \
-b 4 \
-p /home/yueyulin/data/all_train_ds_v1+magpie/ \
-o /home/yueyulin/model/qwen_0.5b_full_layers_stage_2 \
-g 1  \
-s 2 \
-R v6 \
-S 3 \
-k /home/yueyulin/model/qwen_0.5b_full_layers/pytorch_model.bin
```

## Stage 3
Stage 3 is almost identical to Stage 2. The only difference is that the maximum sequence length is set to the length you want to train. The example command is:
```bash
sh 2_train_kl.sh \
-c configs/qwen_0.5b.yaml \
-l 0.0001 \
-f 0.00001 \
-b 4 \
-p /home/yueyulin/data/all_train_ds_v1+magpie/ \
-o /home/yueyulin/model/qwen_0.5b_full_layers_stage_3 \
-g 1  \
-s 2 \
-R v6 \
-S 3 \
-m 4096 \
-k /home/yueyulin/model/qwen_0.5b_full_layers_stage_2/qwen_0.5b_full_layers/pytorch_model.bin
```