
[2024-09-17 08:42:22,433] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 08:53:07,768] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
saving trainable to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072
200 1.306689 3.6939  0, now saving...
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_0_step_200 pretrained from Llama3.18BInstructRWKV8Layers
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_0_step_200/Llama3.18BInstructRWKV8Layers.pth
/home/rwkv/anaconda3/envs/torch_env/lib/python3.11/site-packages/pytorch_lightning/strategies/deepspeed.py:634: When saving the DeepSpeed Stage 3 checkpoint, each worker will save a shard of the checkpoint within a directory. If a single file is required after training, see https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#deepspeed-zero-stage-3-single-file for instructions.
[2024-09-17 09:18:38,009] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
saving trainable to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072
400 1.266554 3.5486  0, now saving...
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_0_step_400 pretrained from Llama3.18BInstructRWKV8Layers
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_0_step_400/Llama3.18BInstructRWKV8Layers.pth
[2024-09-17 09:32:59,109] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
saving trainable to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072
600 1.386127 3.9993  0, now saving...
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_0_step_600 pretrained from Llama3.18BInstructRWKV8Layers
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_0_step_600/Llama3.18BInstructRWKV8Layers.pth
[2024-09-17 10:19:28,858] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
saving trainable to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072
800 1.533519 4.6345  1, now saving...
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_1_step_800 pretrained from Llama3.18BInstructRWKV8Layers
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_1_step_800/Llama3.18BInstructRWKV8Layers.pth
[2024-09-17 10:28:11,156] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 10:46:17,062] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
saving trainable to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072
1000 1.519590 4.5704  1, now saving...
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_1_step_1000 pretrained from Llama3.18BInstructRWKV8Layers
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_1_step_1000/Llama3.18BInstructRWKV8Layers.pth
[2024-09-17 10:51:46,780] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 10:51:54,345] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 10:52:02,099] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 10:57:29,774] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 11:07:37,447] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
saving trainable to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072
1200 1.439721 4.2195  1, now saving...
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_1_step_1200 pretrained from Llama3.18BInstructRWKV8Layers
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_1_step_1200/Llama3.18BInstructRWKV8Layers.pth
[2024-09-17 11:17:37,361] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 11:18:16,304] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 11:22:49,543] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 11:25:10,318] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 11:29:19,454] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
saving trainable to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072
1400 1.233271 3.4324  1, now saving...
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_1_step_1400 pretrained from Llama3.18BInstructRWKV8Layers
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_1_step_1400/Llama3.18BInstructRWKV8Layers.pth
[2024-09-17 11:46:14,692] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 11:50:01,316] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 12:02:18,330] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
saving trainable to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072
1600 1.223657 3.3996  2, now saving...
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_2_step_1600 pretrained from Llama3.18BInstructRWKV8Layers
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_2_step_1600/Llama3.18BInstructRWKV8Layers.pth
[2024-09-17 12:33:17,145] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
saving trainable to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072
1800 1.091117 2.9776  2, now saving...
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_2_step_1800 pretrained from Llama3.18BInstructRWKV8Layers
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_2_step_1800/Llama3.18BInstructRWKV8Layers.pth
[2024-09-17 12:36:18,346] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 12:36:26,051] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
saving trainable to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072
2000 1.216439 3.3751  2, now saving...
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_2_step_2000 pretrained from Llama3.18BInstructRWKV8Layers
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_2_step_2000/Llama3.18BInstructRWKV8Layers.pth
[2024-09-17 13:14:05,389] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
saving trainable to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072
2200 1.189967 3.2870  2, now saving...
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_2_step_2200 pretrained from Llama3.18BInstructRWKV8Layers
save trainable parameters to /data/rwkv/tmp/distill_ultrachat_llamamlp_step_1k_2049_3072/epoch_2_step_2200/Llama3.18BInstructRWKV8Layers.pth
[2024-09-17 13:35:21,950] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 13:35:29,589] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 13:35:37,129] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 13:35:44,688] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 13:35:52,122] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 13:36:00,088] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 13:44:45,008] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2024-09-17 13:44:52,672] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
Detected KeyboardInterrupt, attempting graceful shutdown ...