HybridModel(
  (model): Qwen2ForCausalLM(
    (model): Qwen2Model(
      (embed_tokens): Embedding(151936, 896)
      (layers): ModuleList(
        (0): RWKVDecoderLayer(
          (block): Block(
            (ln1): LayerNorm((896,), eps=1e-05, elementwise_affine=True)
            (ln2): LayerNorm((896,), eps=1e-05, elementwise_affine=True)
            (ln0): LayerNorm((896,), eps=1e-05, elementwise_affine=True)
            (att): RWKV_Tmix_x070(
              (time_shift): ZeroPad2d((0, 0, 1, -1))
              (receptance): Linear(in_features=896, out_features=896, bias=False)
              (key): Linear(in_features=896, out_features=896, bias=False)
              (value): Linear(in_features=896, out_features=896, bias=False)
              (output): Linear(in_features=896, out_features=896, bias=False)
              (ln_x): GroupNorm(14, 896, eps=0.00064, affine=True)
            )
            (ffn): Qwen2MLP(
              (gate_proj): Linear(in_features=896, out_features=4864, bias=False)
              (up_proj): Linear(in_features=896, out_features=4864, bias=False)
              (down_proj): Linear(in_features=4864, out_features=896, bias=False)
              (act_fn): SiLU()
            )
          )
        )
        (1-23): 23 x Qwen2DecoderLayer(
          (self_attn): Qwen2SdpaAttention(
            (q_proj): Linear(in_features=896, out_features=896, bias=True)
            (k_proj): Linear(in_features=896, out_features=128, bias=True)
            (v_proj): Linear(in_features=896, out_features=128, bias=True)
            (o_proj): Linear(in_features=896, out_features=896, bias=False)
            (rotary_emb): Qwen2RotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=896, out_features=4864, bias=False)
            (up_proj): Linear(in_features=896, out_features=4864, bias=False)
            (down_proj): Linear(in_features=4864, out_features=896, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((0,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((0,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((0,), eps=1e-06)
    )
    (lm_head): Linear(in_features=896, out_features=151936, bias=False)
  )
  (teacher_model): Qwen2ForCausalLM(
    (model): Qwen2Model(
      (embed_tokens): Embedding(151936, 896)
      (layers): ModuleList(
        (0-23): 24 x Qwen2DecoderLayer(
          (self_attn): Qwen2FlashAttention2(
            (q_proj): Linear(in_features=896, out_features=896, bias=True)
            (k_proj): Linear(in_features=896, out_features=128, bias=True)
            (v_proj): Linear(in_features=896, out_features=128, bias=True)
            (o_proj): Linear(in_features=896, out_features=896, bias=False)
            (rotary_emb): Qwen2RotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=896, out_features=4864, bias=False)
            (up_proj): Linear(in_features=896, out_features=4864, bias=False)
            (down_proj): Linear(in_features=4864, out_features=896, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((0,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((0,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((0,), eps=1e-06)
    )
    (lm_head): Linear(in_features=896, out_features=151936, bias=False)
  )
)
Estimated memory needed for params, optim states and gradients for a:
HW: Setup with 1 node, 4 GPUs per node.
SW: Model with 0M total params, 0M largest layer params.
  per CPU  |  per GPU |   Options
    0.00GB |   0.00GB | offload_param=OffloadDeviceEnum.cpu, offload_optimizer=OffloadDeviceEnum.cpu, zero_init=1
    0.00GB |   0.00GB | offload_param=OffloadDeviceEnum.cpu, offload_optimizer=OffloadDeviceEnum.cpu, zero_init=0
    0.00GB |   0.00GB | offload_param=none, offload_optimizer=OffloadDeviceEnum.cpu, zero_init=1
    0.00GB |   0.00GB | offload_param=none, offload_optimizer=OffloadDeviceEnum.cpu, zero_init=0
    0.00GB |   0.00GB | offload_param=none, offload_optimizer=none, zero_init=1
    0.00GB |   0.00GB | offload_param=none, offload_optimizer=none, zero_init=0






































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Epoch 0:   0%|▏                                          | 2499/841778 [1:09:42<398:46:03,  1.71s/it, loss=28.7723, steps/s=2.41, kt/s=19.74]

Epoch 0:   0%|▏                                          | 2500/841778 [1:09:44<398:46:29,  1.71s/it, loss=23.9399, steps/s=2.18, kt/s=17.89]
